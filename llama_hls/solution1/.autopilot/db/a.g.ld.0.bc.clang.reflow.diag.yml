--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 88, Column: 2 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.14)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 90, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'compute_rmsnorm(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 20, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 91, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.15)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 409, Column: 2 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_
Args:            
  - Callee:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 90, Column: 2 }
Function:        kernel_matmul
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.19)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 91, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'load_mat(float*, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 92, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'compute_matmul(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 33, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 93, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.20)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 442, Column: 2 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii
Args:            
  - Callee:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 136, Column: 2 }
Function:        kernel_rope
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 140, Column: 3 }
Function:        kernel_rope
Args:            
  - Callee:          'compute_rope(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 142, Column: 3 }
Function:        kernel_rope
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.10)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 427, Column: 9 }
Function:        _ZN4swan11rope_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 95, Column: 2 }
Function:        kernel_softmax
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.4)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 96, Column: 3 }
Function:        kernel_softmax
Args:            
  - Callee:          'compute_softmax(hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 19, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 97, Column: 3 }
Function:        kernel_softmax
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.5)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 66, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 466, Column: 2 }
Function:        _ZN4swan14softmax_kernelEPfS0_i
Args:            
  - Callee:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::softmax_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 283, Column: 29 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - Callee:          'hls::sqrtf(float)'
    DebugLoc:        { File: 'C:\scratch\2025.1\hls_product\640\2025.1\src\shared\hls\clib\hlsmath\src\c++\sqrtfloat.cpp', 
                       Line: 11, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 328, Column: 2 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - Callee:          'swan::softmax_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 223, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 226, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 231, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 234, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 240, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 69, Column: 2 }
Function:        kernel_silu
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 70, Column: 3 }
Function:        kernel_silu
Args:            
  - Callee:          'compute_silu(hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 71, Column: 3 }
Function:        kernel_silu
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 45, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 454, Column: 2 }
Function:        _ZN4swan22silu_activation_kernelEPfS0_i
Args:            
  - Callee:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::silu_activation_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 379, Column: 2 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 382, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 388, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::silu_activation_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 129, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 154, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 172, Column: 5 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 350, Column: 27 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        VITIS_LOOP_350_2
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 317, Column: 26 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        dot_product
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 100, Column: 3 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi.10
Args:            
  - String:          'Loop '''
  - LoopName:        mem_wr
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 88, Column: 17 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Loop '''
  - LoopName:        store_output
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 66, Column: 19 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Loop '''
  - LoopName:        rope_transform
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 58, Column: 16 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Loop '''
  - LoopName:        init_output
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 50, Column: 16 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Loop '''
  - LoopName:        load_cossin
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 43, Column: 11 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Loop '''
  - LoopName:        load_qk
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 11, Column: 3 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9
Args:            
  - String:          'Loop '''
  - LoopName:        mem_rd
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 6, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi
Args:            
  - String:          'Updating loop '
  - String:          lower
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:9:3'
  - String:          ')'
  - String:          ' in function '''
  - String:          load_vec
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 6, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi
Args:            
  - String:          'Updating loop '
  - String:          upper
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:9:3'
  - String:          ')'
  - String:          ' in function '''
  - String:          load_vec
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          lower
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        store_data
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:37:15'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          upper
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        store_data
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:37:15'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          lower
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        compute_activation
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:29:23'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          upper
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        compute_activation
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:29:23'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          lower
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        load_data
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:23:13'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Updating loop '
  - String:          upper
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        load_data
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:23:13'
  - String:          ')'
  - String:          ' in function '''
  - String:          compute_silu
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 45, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi
Args:            
  - String:          'Updating loop '
  - String:          lower
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        mem_wr
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:48:3'
  - String:          ')'
  - String:          ' in function '''
  - String:          store_result
  - String:          '''.'
...
--- !Missed
Pass:            reflow-infer-loop-tripcount
Name:            UpdateLoopBound
DebugLoc:        { File: kernel_silu.cpp, Line: 45, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi
Args:            
  - String:          'Updating loop '
  - String:          upper
  - String:          ' bound from '
  - OldBound:        '2048'
  - String:          ' to '
  - NewBound:        '3072'
  - String:          ' for loop '''
  - LoopName:        mem_wr
  - String:          ''' ('
  - LoopLoc:         'kernel_silu.cpp:48:3'
  - String:          ')'
  - String:          ' in function '''
  - String:          store_result
  - String:          '''.'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaPartiallyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:186:22'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::llama_inference_hls_top'
  - String:          ''' partially '
  - String:          ''
  - String:          'with a factor of '
  - Factor:          '4'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        VITIS_LOOP_350_2
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:350:27'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        parallel_heads
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:305:21'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '12'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        dot_product
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:317:26'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '64'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        split_q_heads
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:295:17'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' completely with a factor of '
  - Factor:          '12'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi.10
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        mem_wr
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:100:3'
  - String:          ') '
  - String:          'in function '''
  - String:          store_result
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi.10
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        mem_wr
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:100:3'
  - String:          ') '
  - String:          'in function '''
  - String:          store_result
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        store_output
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:88:17'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        store_output
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:88:17'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        rope_transform
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:66:19'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' completely with a factor of '
  - Factor:          '32'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        rope_transform
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:66:19'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        init_output
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:58:16'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        init_output
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:58:16'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        load_cossin
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:50:16'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' completely with a factor of '
  - Factor:          '32'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        load_cossin
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:50:16'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        load_qk
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:43:11'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        load_qk
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:43:11'
  - String:          ') '
  - String:          'in function '''
  - String:          compute_rope
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Failure
Pass:            reflow-function-uniquification
Name:            FunctionUniquification
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 89, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - String:          'Duplicating function '''
  - Name:            'load_vec(float*, hls::stream<float, 0>&, int) (.14)'
  - String:          ''' as different function signatures were detected between this call site and other call site(s). This may impact the resources used in the design.'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 88, Column: 2 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.14)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 89, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.14.53)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 90, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'compute_rmsnorm(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 20, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 91, Column: 3 }
Function:        kernel_rmsnorm
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.15)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 409, Column: 2 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_
Args:            
  - Callee:          kernel_rmsnorm
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 90, Column: 2 }
Function:        kernel_matmul
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.19)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 91, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'load_mat(float*, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 92, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'compute_matmul(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 33, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 93, Column: 3 }
Function:        kernel_matmul
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.20)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 442, Column: 2 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii
Args:            
  - Callee:          kernel_matmul
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 136, Column: 2 }
Function:        kernel_rope
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 140, Column: 3 }
Function:        kernel_rope
Args:            
  - Callee:          'compute_rope(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 142, Column: 3 }
Function:        kernel_rope
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.10)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 427, Column: 9 }
Function:        _ZN4swan11rope_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          kernel_rope
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 95, Column: 2 }
Function:        kernel_softmax
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.4)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 96, Column: 3 }
Function:        kernel_softmax
Args:            
  - Callee:          'compute_softmax(hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 19, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 97, Column: 3 }
Function:        kernel_softmax
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.5)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 66, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 466, Column: 2 }
Function:        _ZN4swan14softmax_kernelEPfS0_i
Args:            
  - Callee:          kernel_softmax
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::softmax_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 328, Column: 2 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - Callee:          'swan::softmax_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 223, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 226, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 231, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 234, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 240, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 69, Column: 2 }
Function:        kernel_silu
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 70, Column: 3 }
Function:        kernel_silu
Args:            
  - Callee:          'compute_silu(hls::stream<float, 0>&, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 71, Column: 3 }
Function:        kernel_silu
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 45, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 454, Column: 2 }
Function:        _ZN4swan22silu_activation_kernelEPfS0_i
Args:            
  - Callee:          kernel_silu
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::silu_activation_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 379, Column: 2 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 382, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 388, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - Callee:          'swan::silu_activation_kernel(float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 129, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 154, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 172, Column: 5 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
Function:        apatb_llama_inference_hls_top_ir
Args:            
  - Callee:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          apatb_llama_inference_hls_top_ir
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.56
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' completely with a factor of '
  - Factor:          '32'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.56
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.55
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.55
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.54
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' completely with a factor of '
  - Factor:          '768'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9.54
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weight_final_norm
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
Function:        kernel_rmsnorm
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_vec_2
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       norm_weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wq
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wk
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wv
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       k_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       v_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       k_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       v_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wo
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       norm_weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w1
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w3
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w2
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
Function:        _ZN4swan11rope_kernelEPfS0_S0_S0_S0_S0_.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       cos_vals
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
Function:        _ZN4swan11rope_kernelEPfS0_S0_S0_S0_S0_.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       sin_vals
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
Function:        kernel_rope.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       cos_vec
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
Function:        kernel_rope.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       sin_vec
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       input
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       output
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_vec
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       o_vec
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       output
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
Function:        kernel_rmsnorm.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_vec_2
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
Function:        kernel_rmsnorm.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       o_vec
  - String:          '''.'
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 237, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE11attn_output
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 212, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi.10
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE6k_rope
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 212, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE6q_rope
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 211, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE1v
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 211, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE1k
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 211, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE1q
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 210, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE12normed_input
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 325, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi.5
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_iE19attn_scores_softmax
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 288, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_iE11attn_scores
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 287, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_iE17attn_output_heads
  - String:          ''':'
  - String:          ' '
  - Mode:            Complete
  - String:          ' partitioning'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 286, Column: 0 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_iE7q_heads
  - String:          ''':'
  - String:          ' '
  - Mode:            Complete
  - String:          ' partitioning'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 370, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_E9gated_out
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 369, Column: 0 }
Function:        _ZL12store_resultPfRN3hls6streamIfLi0EEEi
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_E8silu_out
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 368, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_E6w3_out
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 367, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_E6w1_out
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 366, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_E12normed_input
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 88, Column: 0 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE11temp_buffer
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 87, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE10ffn_output
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 86, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE11attn_output
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 85, Column: 0 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_iiE12hidden_state
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_matmul.cpp, Line: 36, Column: 0 }
Function:        _ZL14compute_matmulRN3hls6streamIfLi0EEES2_S2_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL14compute_matmulRN3hls6streamIfLi0EEES2_S2_iiE9vec_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 24, Column: 0 }
Function:        _ZL15compute_rmsnormRN3hls6streamIfLi0EEES2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL15compute_rmsnormRN3hls6streamIfLi0EEES2_S2_iE11vec_local_2
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 23, Column: 0 }
Function:        _ZL15compute_rmsnormRN3hls6streamIfLi0EEES2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL15compute_rmsnormRN3hls6streamIfLi0EEES2_S2_iE11vec_local_1
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 32, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE11k_out_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 31, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE11q_out_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 30, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE9sin_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '2'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 29, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE9cos_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '2'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 28, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE7k_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_rope.cpp, Line: 27, Column: 0 }
Function:        _ZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_ropeRN3hls6streamIfLi0EEES2_S2_S2_S2_S2_iE7q_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_softmax.cpp, Line: 23, Column: 0 }
Function:        _ZL15compute_softmaxRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL15compute_softmaxRN3hls6streamIfLi0EEES2_iE11vec_local_2
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_softmax.cpp, Line: 22, Column: 0 }
Function:        _ZL15compute_softmaxRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL15compute_softmaxRN3hls6streamIfLi0EEES2_iE11vec_local_1
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: kernel_silu.cpp, Line: 20, Column: 0 }
Function:        _ZL12compute_siluRN3hls6streamIfLi0EEES2_i
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          _ZZL12compute_siluRN3hls6streamIfLi0EEES2_iE9vec_local
  - String:          ''':'
  - String:          ' '
  - Mode:            Cyclic
  - String:          ' partitioning'
  - String:          ' with factor '
  - Factor:          '4'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 116, Column: 15 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          cos_vals
  - String:          ''':'
  - String:          ' '
  - Mode:            Complete
  - String:          ' partitioning'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-array-partition
Name:            ArrayXform
DebugLoc:        { File: llama_hls_top.cpp, Line: 117, Column: 15 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Applying '
  - String:          array_partition
  - String:          ' to '''
  - UOName:          sin_vals
  - String:          ''':'
  - String:          ' '
  - Mode:            Complete
  - String:          ' partitioning'
  - String:          ' on dimension '
  - Dim:             '1'
  - String:          .
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
Function:        _ZL8load_matPfRN3hls6streamIfLi0EEEii
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of '
  - Length:          variable
  - String:          ' length'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgepseq
  - ArrayName:       i_mat
  - String:          ' '
  - BundleName:      gmem7,gmem9,gmem8,gmem3,gmem4,gmem5,gmem6
  - String:          ' '
  - LoopName:        VITIS_LOOP_22_1
  - String:          ' '
  - LoopLoc:         'kernel_matmul.cpp:22:19'
  - String:          ' '
  - Function:        'load_mat(float*, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: kernel_matmul.cpp, Line: 25, Column: 11 }
  - OrigDirection:   read
  - OrigID:          for.inc.load.7
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
Function:        _ZL8load_matPfRN3hls6streamIfLi0EEEii
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       i_mat
  - String:          ' '
  - BundleName:      gmem7,gmem9,gmem8,gmem3,gmem4,gmem5,gmem6
  - String:          ' '
  - LoopName:        mem_rd
  - String:          ' '
  - LoopLoc:         'kernel_matmul.cpp:20:3'
  - String:          ' '
  - Function:        'load_mat(float*, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 97, Column: 19 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '768'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        input_embeddingseq
  - ArrayName:       input_embedding
  - String:          ' '
  - BundleName:      gmem0
  - String:          ' '
  - LoopName:        load_input_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:97:19'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 100, Column: 20 }
  - OrigDirection:   read
  - OrigID:          for.inc.load.10
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '32'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgepseq
  - ArrayName:       cos_table
  - String:          ' '
  - BundleName:      gmem15
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 124, Column: 16 }
  - OrigDirection:   read
  - OrigID:          for.inc31.load.5
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '32'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep154seq
  - ArrayName:       sin_table
  - String:          ' '
  - BundleName:      gmem16
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 125, Column: 27 }
  - OrigDirection:   read
  - OrigID:          for.inc31.load.8
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '24576000'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        weight_token_embeddingseq
  - ArrayName:       weight_token_embedding
  - String:          ' '
  - BundleName:      gmem2
  - String:          ' '
  - LoopName:        compute_logits
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 189, Column: 27 }
  - OrigDirection:   read
  - OrigID:          for.inc119.load.17
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 189, Column: 27 }
  - OrigDirection:   read
  - OrigID:          for.inc119.load.27
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 189, Column: 27 }
  - OrigDirection:   read
  - OrigID:          for.inc119.load.37
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 189, Column: 27 }
  - OrigDirection:   read
  - OrigID:          for.inc119.load.47
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Sequential '
  - Direction:       write
  - String:          ' of length '
  - Length:          '32000'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        output_logitsseq
  - ArrayName:       output_logits
  - String:          ' '
  - BundleName:      gmem1
  - String:          ' '
  - LoopName:        compute_logits
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      store
    DebugLoc:        { File: llama_hls_top.cpp, Line: 191, Column: 26 }
  - OrigDirection:   write
  - OrigID:          for.inc124.store.3
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       cos_table
  - String:          ' '
  - BundleName:      gmem15
  - String:          ' '
  - LoopName:        transformer_layers
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:104:25'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
  - OrigDirection:   read
  - OrigID:          scevgep154seq
  - ArrayName:       sin_table
  - String:          ' '
  - BundleName:      gmem16
  - String:          ' '
  - LoopName:        transformer_layers
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:104:25'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 259, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       write
  - String:          ' of length '
  - Length:          '768'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgepseq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        update_k
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:259:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      store
    DebugLoc:        { File: llama_hls_top.cpp, Line: 262, Column: 28 }
  - OrigDirection:   write
  - OrigID:          for.inc.store.16
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 266, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       write
  - String:          ' of length '
  - Length:          '768'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep5seq
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        update_v
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:266:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      store
    DebugLoc:        { File: llama_hls_top.cpp, Line: 269, Column: 28 }
  - OrigDirection:   write
  - OrigID:          for.inc13.store.16
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgepseq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.78
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.85
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.92
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.99
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.106
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.113
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.120
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.127
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.134
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.141
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.148
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.155
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.162
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.169
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.176
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.183
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.190
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.197
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.204
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.211
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.218
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.225
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.232
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.239
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.246
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.253
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.260
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.267
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.274
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.281
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.288
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.295
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.302
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.309
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.316
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.323
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.330
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.337
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.344
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.351
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.358
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.365
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.372
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.379
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.386
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.393
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.400
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.407
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.414
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.421
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.428
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.435
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.442
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.449
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.456
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.463
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.470
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.477
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.484
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.491
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.498
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.505
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.512
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.load.519
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep916seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.1.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep917seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.2.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep918seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.3.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep919seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.4.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep920seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.5.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep921seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.6.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep922seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.7.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep923seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.8.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep924seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.9.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep925seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.10.load.521
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '64'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        scevgep926seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.80
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.87
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.94
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.101
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.108
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.115
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.122
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.129
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.136
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.143
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.150
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.157
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.164
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.171
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.178
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.185
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.192
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.199
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.206
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.213
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.220
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.227
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.234
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.241
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.248
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.255
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.262
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.269
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.276
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.283
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.290
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.297
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.304
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.311
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.318
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.325
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.332
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.339
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.346
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.353
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.360
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.367
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.374
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.381
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.388
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.395
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.402
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.409
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.416
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.423
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.430
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.437
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.444
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.451
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.458
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.465
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.472
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.479
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.486
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.493
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.500
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.507
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.514
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          dot_product.11.load.521
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.load.18
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep916seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.1.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep917seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.2.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep918seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.3.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep919seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.4.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep920seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.5.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep921seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.6.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep922seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.7.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep923seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.8.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep924seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.9.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep925seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.10.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep926seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        compute_scores
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:309:18'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-inference
Name:            IncompatibleStride
DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          Stride is incompatible
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: llama_hls_top.cpp, Line: 340, Column: 60 }
  - OrigDirection:   read
  - OrigID:          for.inc69.11.load.19
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        weighted_sum_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-inference
Name:            BurstInferred
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 11, Column: 3 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.14.53
Args:            
  - String:          'Sequential '
  - Direction:       read
  - String:          ' of length '
  - Length:          '768'
  - String:          ' has been inferred'
  - String:          ' _XLX_SEP_ '
  - AccessID:        i_vecseq
  - ArrayName:       i_vec
  - String:          ' '
  - BundleName:      gmem12,gmem11,gmem10
  - String:          ' '
  - LoopName:        mem_rd
  - String:          ' '
  - LoopLoc:         'kernel_rmsnorm.cpp:11:3'
  - String:          ' '
  - Function:        'load_vec(float*, hls::stream<float, 0>&, int) (.14.53)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' '
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      load
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 14, Column: 11 }
  - OrigDirection:   read
  - OrigID:          for.inc.load.4
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
Function:        _ZL8load_matPfRN3hls6streamIfLi0EEEii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       i_mat
  - String:          ' '
  - BundleName:      gmem7,gmem9,gmem8,gmem3,gmem4,gmem5,gmem6
  - String:          ' '
  - LoopName:        VITIS_LOOP_22_1
  - String:          ' '
  - LoopLoc:         'kernel_matmul.cpp:22:19'
  - String:          ' '
  - Function:        'load_mat(float*, hls::stream<float, 0>&, int, int)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
  - OrigDirection:   write
  - OrigID:          output_logitsseq
  - ArrayName:       output_logits
  - String:          ' '
  - BundleName:      gmem1
  - String:          ' '
  - LoopName:        compute_logits
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
  - OrigDirection:   read
  - OrigID:          weight_token_embeddingseq
  - ArrayName:       weight_token_embedding
  - String:          ' '
  - BundleName:      gmem2
  - String:          ' '
  - LoopName:        dot_product
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:186:22'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       cos_table
  - String:          ' '
  - BundleName:      gmem15
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
  - OrigDirection:   read
  - OrigID:          scevgep154seq
  - ArrayName:       sin_table
  - String:          ' '
  - BundleName:      gmem16
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 97, Column: 19 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 97, Column: 19 }
  - OrigDirection:   read
  - OrigID:          input_embeddingseq
  - ArrayName:       input_embedding
  - String:          ' '
  - BundleName:      gmem0
  - String:          ' '
  - LoopName:        load_input_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:97:19'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 266, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 266, Column: 15 }
  - OrigDirection:   write
  - OrigID:          scevgep5seq
  - ArrayName:       v_cache
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        update_v
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:266:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 259, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 259, Column: 15 }
  - OrigDirection:   write
  - OrigID:          scevgepseq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        update_k
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:259:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep926seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep925seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep924seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep923seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep922seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep921seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep920seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep919seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep918seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep917seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgep916seq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
  - OrigDirection:   read
  - OrigID:          scevgepseq
  - ArrayName:       k_cache
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Missed
Pass:            reflow-burst-widen
Name:            GreaterOrEqualThreshold
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 11, Column: 3 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.14.53
Args:            
  - String:          'Could not widen since type '
  - Type:            float
  - String:          ' size is greater than or equal to the max_widen_bitwidth threshold of '
  - Threshold:       '0'
  - String:          ' _XLX_SEP_ '
  - OrigAccess:      call
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 11, Column: 3 }
  - OrigDirection:   read
  - OrigID:          i_vecseq
  - ArrayName:       i_vec
  - String:          ' '
  - BundleName:      gmem12,gmem11,gmem10
  - String:          ' '
  - LoopName:        mem_rd
  - String:          ' '
  - LoopLoc:         'kernel_rmsnorm.cpp:11:3'
  - String:          ' '
  - Function:        'load_vec(float*, hls::stream<float, 0>&, int) (.14.53)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFullyUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9
Args:            
  - String:          'Unrolling loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' completely with a factor of '
  - Factor:          '32'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PipelineRemovedByUnroll
DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.9
Args:            
  - String:          'Pipeline directive for loop '''
  - LoopName:        mem_rd
  - String:          ''' ('
  - LoopLoc:         'kernel_rope.cpp:11:3'
  - String:          ') '
  - String:          'in function '''
  - String:          load_vec
  - String:          ''' has been removed because the loop is unrolled completely'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !HLSReportInfo
Pass:            auto-loop-pipeline
Name:            AutoLoopPipeline
Args:            
  - String:          'automatically set the pipeline for Loop< '
  - LoopName:        dot_product
  - String:          '> at '
  - LoopLoc:         'llama_hls_top.cpp:186:22'
  - String:          ' '
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w1
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w3
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       w2
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul.clone.clone.99.102
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wq
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wk
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wv
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       k_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       v_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       k_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       v_cache
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       wo
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       weights
  - String:          '''.'
...
--- !Failure
Pass:            reflow-array-undecay
Name:            WarnOnUnsupportedVLAUnDecay
DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
Function:        kernel_matmul
Args:            
  - String:          'Skipping array undecay on variable length array '''
  - ArrayName:       i_mat
  - String:          '''.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 11, Column: 3 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.14.53.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem12
  - String:          ' '
  - LoopName:        mem_rd
  - String:          ' '
  - LoopLoc:         'kernel_rmsnorm.cpp:11:3'
  - String:          ' '
  - Function:        'load_vec(float*, hls::stream<float, 0>&, int) (.14.53.1)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 11, Column: 3 }
Function:        _ZL8load_vecPfRN3hls6streamIfLi0EEEi.14.53.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        mem_rd
  - String:          '''('
  - LoopLoc:         'kernel_rmsnorm.cpp:11:3'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem12
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
Function:        _ZL8load_matPfRN3hls6streamIfLi0EEEii.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of '
  - Length:          variable
  - String:          ' length and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem8
  - String:          ' '
  - LoopName:        VITIS_LOOP_22_1
  - String:          ' '
  - LoopLoc:         'kernel_matmul.cpp:22:19'
  - String:          ' '
  - Function:        'load_mat(float*, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: kernel_matmul.cpp, Line: 22, Column: 19 }
Function:        _ZL8load_matPfRN3hls6streamIfLi0EEEii.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of '
  - Length:          variable
  - String:          ' length and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        VITIS_LOOP_22_1
  - String:          '''('
  - LoopLoc:         'kernel_matmul.cpp:22:19'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem8
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 259, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - LoopName:        update_k
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:259:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 259, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        update_k
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:259:15'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 266, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq1
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem14
  - String:          ' '
  - LoopName:        update_v
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:266:15'
  - String:          ' '
  - Function:        'swan::update_kv_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 266, Column: 15 }
Function:        _ZN4swan22update_kv_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        update_v
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:266:15'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem14
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq1
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq2
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq3
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq4
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq5
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq6
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq7
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq8
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq9
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq10
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq11
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem13
  - String:          ' '
  - Function:        'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 319, Column: 30 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '64'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem13
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 97, Column: 19 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem0
  - String:          ' '
  - LoopName:        load_input_loop
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:97:19'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 97, Column: 19 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '768'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        load_input_loop
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:97:19'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem0
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '32'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq1
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem15
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '32'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        rope_load
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem15
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '32'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq2
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem16
  - String:          ' '
  - LoopName:        rope_load
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 121, Column: 13 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '32'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        rope_load
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:121:13'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem16
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '24576000'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq3
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem2
  - String:          ' '
  - LoopName:        compute_logits
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       reads
  - String:          ' of length '
  - Length:          '24576000'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        compute_logits
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem2
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredVerboseSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '32000'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' has been inferred.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
  - String:          ' _XLX_SEP_ '
  - AccessID:        seq4
  - ArrayName:       ''
  - String:          ' '
  - BundleName:      gmem1
  - String:          ' '
  - LoopName:        compute_logits
  - String:          ' '
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ' '
  - Function:        'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' '
...
--- !Passed
Pass:            reflow-burst-summary
Name:            BurstInferredSummary
DebugLoc:        { File: llama_hls_top.cpp, Line: 182, Column: 21 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - String:          'Multiple burst '
  - Direction:       writes
  - String:          ' of length '
  - Length:          '32000'
  - String:          ' and bit width '
  - Width:           '32'
  - String:          ' in loop '''
  - LoopName:        compute_logits
  - String:          '''('
  - LoopLoc:         'llama_hls_top.cpp:182:21'
  - String:          ')'
  - String:          ' has been inferred on bundle '''
  - BundleName:      gmem1
  - String:          '''.'
  - String:          ' These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Passed
Pass:            reflow-infer-unroll
Name:            ImplicitUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' is marked as complete unroll implied by the pipeline pragma'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            reflow-pragma-loop-unroll
Name:            PragmaFailFullyUnroll
DebugLoc:        { File: llama_hls_top.cpp, Line: 336, Column: 32 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - String:          'Cannot unroll loop '''
  - LoopName:        weighted_sum_loop
  - String:          ''' ('
  - LoopLoc:         'llama_hls_top.cpp:336:32'
  - String:          ') '
  - String:          'in function '''
  - String:          'swan::compute_attention_with_cache_kernel'
  - String:          ''' as it has a variable trip count'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 88, Column: 2 }
Function:        kernel_rmsnorm.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.14.1)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm.1
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 89, Column: 3 }
Function:        kernel_rmsnorm.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.14.53.1)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm.1
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 90, Column: 3 }
Function:        kernel_rmsnorm.1
Args:            
  - Callee:          'compute_rmsnorm(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 20, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm.1
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 91, Column: 3 }
Function:        kernel_rmsnorm.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.15.1)'
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rmsnorm.1
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 409, Column: 2 }
Function:        _ZN4swan14rmsnorm_kernelEPfS0_S0_.1
Args:            
  - Callee:          kernel_rmsnorm.1
    DebugLoc:        { File: kernel_rmsnorm.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rmsnorm_kernel(float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 90, Column: 2 }
Function:        kernel_matmul.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.19.86.87.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 91, Column: 3 }
Function:        kernel_matmul.1
Args:            
  - Callee:          'load_mat(float*, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 92, Column: 3 }
Function:        kernel_matmul.1
Args:            
  - Callee:          'compute_matmul(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 33, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 93, Column: 3 }
Function:        kernel_matmul.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.20.88.89.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 442, Column: 2 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.1
Args:            
  - Callee:          kernel_matmul.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::matmul_kernel(float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 136, Column: 2 }
Function:        kernel_rope.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9.54.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 137, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9.55.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 138, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9.56.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 139, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.9.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 8, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 140, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'compute_rope(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 23, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 142, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.10.57.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_rope.cpp, Line: 143, Column: 3 }
Function:        kernel_rope.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.10.1)'
    DebugLoc:        { File: kernel_rope.cpp, Line: 97, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 427, Column: 9 }
Function:        _ZN4swan11rope_kernelEPfS0_S0_S0_S0_S0_.1
Args:            
  - Callee:          kernel_rope.1
    DebugLoc:        { File: kernel_rope.cpp, Line: 109, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 91, Column: 30 }
Function:        kernel_softmax.1
Args:            
  - Callee:          kernel_softmax.1_Block_entry_proc
    DebugLoc:        { File: kernel_softmax.cpp, Line: 0, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax.1
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 95, Column: 2 }
Function:        kernel_softmax.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.4.1)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 0, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax.1
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 96, Column: 3 }
Function:        kernel_softmax.1
Args:            
  - Callee:          'compute_softmax(hls::stream<float, 0>&, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 0, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax.1
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_softmax.cpp, Line: 97, Column: 3 }
Function:        kernel_softmax.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.5.1)'
    DebugLoc:        { File: kernel_softmax.cpp, Line: 0, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_softmax.1
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 466, Column: 2 }
Function:        _ZN4swan14softmax_kernelEPfS0_i.1
Args:            
  - Callee:          kernel_softmax.1
    DebugLoc:        { File: kernel_softmax.cpp, Line: 77, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::softmax_kernel(float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 328, Column: 2 }
Function:        _ZN4swan35compute_attention_with_cache_kernelEPfS0_S0_S0_i.1
Args:            
  - Callee:          'swan::softmax_kernel(float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 462, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 223, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii.1
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 226, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii.1
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 231, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii.1
Args:            
  - Callee:          'swan::rope_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 420, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 234, Column: 5 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii.1
Args:            
  - Callee:          'swan::update_kv_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 253, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 240, Column: 2 }
Function:        _ZN4swan33attention_layer_with_cache_kernelEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii.1
Args:            
  - Callee:          'swan::compute_attention_with_cache_kernel(float*, float*, float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 280, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 90, Column: 2 }
Function:        kernel_matmul.clone.clone.99.102.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.19.86.87.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.clone.99.102.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 91, Column: 3 }
Function:        kernel_matmul.clone.clone.99.102.1
Args:            
  - Callee:          'load_mat(float*, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.clone.99.102.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 92, Column: 3 }
Function:        kernel_matmul.clone.clone.99.102.1
Args:            
  - Callee:          'compute_matmul(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 33, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.clone.99.102.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 93, Column: 3 }
Function:        kernel_matmul.clone.clone.99.102.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.20.clone.100.101.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.clone.99.102.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 442, Column: 2 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.1
Args:            
  - Callee:          kernel_matmul.clone.clone.99.102.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::matmul_kernel(float*, float*, float*, int, int) (.clone.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 69, Column: 2 }
Function:        kernel_silu.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu.1
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 70, Column: 3 }
Function:        kernel_silu.1
Args:            
  - Callee:          'compute_silu(hls::stream<float, 0>&, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu.1
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_silu.cpp, Line: 71, Column: 3 }
Function:        kernel_silu.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.1)'
    DebugLoc:        { File: kernel_silu.cpp, Line: 45, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_silu.1
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 454, Column: 2 }
Function:        _ZN4swan22silu_activation_kernelEPfS0_i.1
Args:            
  - Callee:          kernel_silu.1
    DebugLoc:        { File: kernel_silu.cpp, Line: 56, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::silu_activation_kernel(float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 90, Column: 2 }
Function:        kernel_matmul.clone.1
Args:            
  - Callee:          'load_vec(float*, hls::stream<float, 0>&, int) (.19.clone.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 6, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 91, Column: 3 }
Function:        kernel_matmul.clone.1
Args:            
  - Callee:          'load_mat(float*, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 17, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 92, Column: 3 }
Function:        kernel_matmul.clone.1
Args:            
  - Callee:          'compute_matmul(hls::stream<float, 0>&, hls::stream<float, 0>&, hls::stream<float, 0>&, int, int) (.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 33, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: kernel_matmul.cpp, Line: 93, Column: 3 }
Function:        kernel_matmul.clone.1
Args:            
  - Callee:          'store_result(float*, hls::stream<float, 0>&, int) (.20.88.89.1)'
    DebugLoc:        { File: kernel_matmul.cpp, Line: 60, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          kernel_matmul.clone.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 442, Column: 2 }
Function:        _ZN4swan13matmul_kernelEPfS0_S0_ii.clone.clone.1
Args:            
  - Callee:          kernel_matmul.clone.1
    DebugLoc:        { File: kernel_matmul.cpp, Line: 72, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::matmul_kernel(float*, float*, float*, int, int) (.clone.clone.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 379, Column: 2 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_.1
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 382, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_.1
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int) (.clone.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 388, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_.1
Args:            
  - Callee:          'swan::silu_activation_kernel(float*, float*, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 450, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 397, Column: 5 }
Function:        _ZN4swan16ffn_layer_kernelEPfS0_S0_S0_S0_S0_.1
Args:            
  - Callee:          'swan::matmul_kernel(float*, float*, float*, int, int) (.clone.clone.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 438, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 129, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::attention_layer_with_cache_kernel(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 206, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 154, Column: 9 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::ffn_layer_kernel(float*, float*, float*, float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 363, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
--- !Missed
Pass:            inline
Name:            NeverInline
DebugLoc:        { File: llama_hls_top.cpp, Line: 172, Column: 5 }
Function:        _ZN4swan23llama_inference_hls_topEPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_ii
Args:            
  - Callee:          'swan::rmsnorm_kernel(float*, float*, float*) (.1)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 405, Column: 0 }
  - String:          ' not inlined into '
  - Caller:          'swan::llama_inference_hls_top(float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, float*, int, int)'
    DebugLoc:        { File: llama_hls_top.cpp, Line: 59, Column: 0 }
  - String:          ' because it should never be inlined (cost=never)'
...
